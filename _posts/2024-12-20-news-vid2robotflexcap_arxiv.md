---
layout: post
categories: news
date: 2024-03-20 00:21:59 +00:00
title:  "Two Preprints out! Work done as student researcher at Google DeepMind Robotics"
titleurl: https://openreview.net/forum?id=Eal_lL08v_l
important: "true"
highlight: "" #ffffd0"
summary: 'We shared the preprints of two papers. 
<br>
<ul>
<li><strong>Vid2Robot</strong>, a novel end-to-end conditioned robot policy with Cross Attention. 

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">What if we could show a robot how to do a task? <br><br>We present Vid2Robot,  which is a robot policy trained to decode human intent from visual cues and translate it into actions in its environment. ðŸ¤–<br><br>Website: <a href="https://t.co/ufFHK1Dgbg">https://t.co/ufFHK1Dgbg</a><br>Arxiv: <a href="https://t.co/qEUjaXovJa">https://t.co/qEUjaXovJa</a><br><br>ðŸ§µ(1/n) <a href="https://t.co/13pgW8ssEY">pic.twitter.com/13pgW8ssEY</a></p>&mdash; Vidhi Jain (@viddivj) <a href="https://twitter.com/viddivj/status/1770301736051483025?ref_src=twsrc%5Etfw">March 20, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<br>
<li><strong>FlexCap</strong>, a foundational visual language model for flexibly captioning any region of the image with controllable granularity.  

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to share our work on FlexCap! It can provide visual captions at varying level of granularity for any region in the image. <br><br>Website: <a href="https://t.co/hJ3WFXGRc7">https://t.co/hJ3WFXGRc7</a><br><br>See ðŸ§µby <a href="https://twitter.com/debidatta?ref_src=twsrc%5Etfw">@debidatta</a> for more details! <a href="https://t.co/sY60Nalb6H">https://t.co/sY60Nalb6H</a></p>&mdash; Vidhi Jain (@viddivj) <a href="https://twitter.com/viddivj/status/1770144272232091853?ref_src=twsrc%5Etfw">March 19, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</ul>'
---
